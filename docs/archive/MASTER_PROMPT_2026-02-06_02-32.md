Music Agent - Master Project Context v2.2
Document Info
Last Updated: February 6, 2026, 1:09 AM CET‚Ä®Status: Milestones 1-3 Complete | Milestone 4 (Storage Manager) NEXT‚Ä®Version: 2.2 (Milestone 4 scope refined, startup commands added)

About Me
I'm learning to code through hands-on building ("vibe coding"). I'm a beginner building a workflow automation system for electronic music releases. I use Cursor IDE with Claude Code, and I'm comfortable with terminal commands but need explanations for technical concepts.
Current Skill Level:
	‚Ä¢	Comfortable with: Terminal basics, file system navigation, Git via GitHub Desktop, copying/pasting code
	‚Ä¢	Learning: Node.js/Express, n8n workflows, API design, async/await, error handling, file I/O
	‚Ä¢	Goal: Understand how systems work together (not just copy-paste, but comprehend WHY)

Current Tech Stack
	‚Ä¢	Backend: Node.js v24.13.0 + Express + Multer (file-handler API on port 3001)
	‚Ä¢	Workflow Engine: n8n (Docker container on localhost:5678)
	‚Ä¢	Storage: Local folders: ~/Documents/Music Agent/Releases/
	‚Ä¢	Development: macOS, Cursor IDE, GitHub Desktop for version control
	‚Ä¢	Future Frontend: Next.js (Milestone 7)

Development Environment Startup
Run these commands at the start of each development session:
Step 1: Start n8n Docker Container

bash
docker start n8n
What this does: Starts the n8n workflow engine (runs in background on port 5678)‚Ä®Verify it's running: Open browser to http://localhost:5678
Step 2: Start File-Handler API

bash
cd ~/Documents/music-agent-mvp/file-handler
node server.js
What this does: Starts the Express server (listens on port 3001)‚Ä®Success message: File-handler API listening on port 3001‚Ä®Keep this terminal window open (server runs in foreground)
Step 3: Verify Health Check

bash
curl http://localhost:3001/health
Expected response: {"status":"ok","timestamp":"..."}‚Ä®If fails: Check that Step 2 is running without errors
Quick Startup Verification Checklist
	‚Ä¢	n8n accessible at http://localhost:5678
	‚Ä¢	File-handler API responds to /health endpoint
	‚Ä¢	~/Documents/Music Agent/Releases/ folder exists
	‚Ä¢	Terminal window with node server.js is running (don't close it)

Project Structure

text
~/Documents/music-agent-mvp/
‚îú‚îÄ‚îÄ file-handler/
‚îÇ   ‚îú‚îÄ‚îÄ server.js (Express API with Multer + metadata endpoint)
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ package-lock.json
‚îÇ   ‚îî‚îÄ‚îÄ node_modules/
‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îî‚îÄ‚îÄ Release_Form_Workflow.json (exported n8n workflow)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ MASTER_PROMPT.md (this file - current version)
‚îÇ   ‚îî‚îÄ‚îÄ archive/
‚îÇ       ‚îú‚îÄ‚îÄ MASTER_PROMPT_2026-02-05_22-03.md
‚îÇ       ‚îî‚îÄ‚îÄ MASTER_PROMPT_2026-02-05_23-33.md
‚îú‚îÄ‚îÄ MILESTONE_1_COMPLETE.md
‚îú‚îÄ‚îÄ MILESTONE_2_COMPLETE.md
‚îú‚îÄ‚îÄ MILESTONE_3_COMPLETE.md
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore

Current State (MVP Progress)
‚úÖ Milestone 1 Complete: Release Intake
	‚Ä¢	n8n "Release Intake" workflow receives JSON metadata via webhook
	‚Ä¢	Transforms metadata (generates releaseId: YYYY-MM-DD_Artist_Title format)
	‚Ä¢	Responds with confirmation
‚úÖ Milestone 2 Complete: File Upload Handler
	‚Ä¢	File-handler API fully functional (server.js in ~/Documents/music-agent-mvp/file-handler/)
	‚Ä¢	Successfully tested: curl + n8n Form uploads work perfectly
	‚Ä¢	Server uses .any() to accept any field name, classify() function sorts by mimetype + file extension fallback
	‚Ä¢	n8n Form Trigger with 7 fields: artist, title, genre (dropdown), releaseType, audioFile, artworkFile, videoFile (optional)
	‚Ä¢	Docker networking working: using host.docker.internal:3001
	‚Ä¢	IF node conditional routing: handles video vs no-video uploads correctly
	‚Ä¢	Tested with real files: audio (56MB WAV), artwork (3MB PNG), video (6MB MP4)
‚úÖ Milestone 3 Complete: Metadata Transformer
	‚Ä¢	Input validation: artist (1-100 chars), title (1-200 chars), genre (dropdown), releaseType (Singles only in MVP)
	‚Ä¢	Genre dropdown: 10 options (Ambient, Deep House, House, Indie Dance, Melodic House and Techno, Progressive House, Tech House, Techno, Trance, Other)
	‚Ä¢	metadata.json generation: releaseId, artist, title, genre, releaseType, releaseDate, createdAt, files array
	‚Ä¢	Files array catalogs: filename, size, mimetype for each uploaded file
	‚Ä¢	Metadata saved to each release folder alongside audio/artwork/video files
	‚Ä¢	Works with and without video uploads
	‚Ä¢	Validation errors provide helpful feedback to user
‚è≥ Milestone 4: Storage Manager (NEXT)
MVP Scope (Core Features Only):
	‚Ä¢	Add release listing/browsing capabilities (GET /releases endpoint)
	‚Ä¢	Implement duplicate detection (prevent accidental re-uploads)
	‚Ä¢	Add file validation (check for corrupt/invalid files before saving)
	‚Ä¢	Add disk space monitoring (warn when storage is low)
Deferred to V2:
	‚Ä¢	Release archiving (move old releases to archive folder)
	‚Ä¢	Release deletion with confirmation
Why This Matters:
	‚Ä¢	As you create more releases (1-2 per week), you need to manage them efficiently
	‚Ä¢	Prevents accidental overwrites of existing releases
	‚Ä¢	Ensures file quality before distribution
	‚Ä¢	Disk space awareness for large WAV files
Recommended Build Order:
	1	Release Listing (Foundation) - GET /releases endpoint, directory scanning
	2	Disk Space Monitoring - Add check-disk-space package, quick win
	3	Duplicate Detection (Simple) - Check if releaseId folder exists
	4	File Validation - Install music-metadata, validate audio files
Required npm Packages:

bash
npm install music-metadata check-disk-space file-type-checker
‚è≥ Milestone 5: Distribution Orchestrator
	‚Ä¢	SoundCloud/YouTube automated uploads
	‚Ä¢	DistroKid/Bandcamp manual guides
‚è≥ Milestone 6: Promo Content Generator
	‚Ä¢	Social media templates
	‚Ä¢	Automated caption generation
‚è≥ Milestone 7: Next.js UI
	‚Ä¢	User-facing upload form
	‚Ä¢	Replace n8n Form Trigger with custom UI

File-Handler API Details
server.js configuration:
	‚Ä¢	Port: 3001
	‚Ä¢	Endpoints:
	‚ó¶	GET /health - Server status check
	‚ó¶	POST /upload?releaseId=...&artist=...&title=...&genre=... - File uploads
	‚ó¶	POST /metadata - Save metadata.json
	‚ó¶	Milestone 4 additions (planned):
	‚ñ™	GET /releases - List all releases
	‚ñ™	GET /releases/:releaseId - Get specific release details
	‚ñ™	GET /storage/status - Disk space information
	‚ó¶	V2 additions (deferred):
	‚ñ™	POST /releases/:releaseId/archive - Archive a release
	‚ñ™	DELETE /releases/:releaseId - Delete a release (with confirmation)
Multer Configuration:
	‚Ä¢	Uses .any() to accept any field name (flexible for different upload sources)
	‚Ä¢	File classification: classify() function checks mimetype first, falls back to file extension
	‚ó¶	Audio extensions: .wav, .mp3, .flac, .aiff, .m4a, .ogg ‚Üí audio/ folder
	‚ó¶	Image extensions: .jpg, .jpeg, .png, .gif, .webp, .bmp ‚Üí artwork/ folder
	‚ó¶	Video extensions: .mp4, .mov, .avi, .mkv, .webm ‚Üí video/ folder
Storage Structure:
	‚Ä¢	Path: RELEASES_BASE/<releaseId>/<audio|artwork|video>/
	‚Ä¢	Example: 2026-02-05_SophieJoe_TellMe/audio/track.wav, /artwork/cover.jpg, /video/promo.mp4, /metadata.json
	‚Ä¢	Why: Keeps all release assets together, easy to zip for distribution, clear organization
Query Parameters:
	‚Ä¢	releaseId, artist, title, genre passed from n8n and returned in upload response
	‚Ä¢	Why: Allows metadata generation without referencing previous nodes by name
Middleware:
	‚Ä¢	CORS enabled for n8n/Next.js calls
	‚Ä¢	express.json() middleware for JSON body parsing (required for POST /metadata)

n8n Workflow Structure (Milestone 3)

text
Form Trigger (7 fields: artist, title, genre dropdown, releaseType, audio, artwork, video)
  ‚Üì
Validate Inputs (Code node: checks required fields, format, genre list)
  ‚Üì
Set Node (generates releaseId: YYYY-MM-DD_ArtistName_TrackTitle, timestamp)
  ‚Üì
IF Node (checks if video file uploaded)
  ‚îú‚îÄ‚Üí HTTP Request (with video) ‚Üí Create Metadata (with video) ‚Üí Save Metadata (with video) ‚Üí Respond
  ‚îî‚îÄ‚Üí HTTP Request (no video) ‚Üí Create Metadata (no video) ‚Üí Save Metadata (no video) ‚Üí Respond
Key Implementation Details:
	‚Ä¢	Form Trigger outputs to Validate node
	‚Ä¢	Set node creates releaseId by removing spaces from artist/title: 2026-02-05_SophieJoe_TellMe
	‚Ä¢	IF node splits into parallel branches (only one executes per submission)
	‚Ä¢	HTTP Request nodes send form data via query parameters to file-handler
	‚Ä¢	Create Metadata nodes extract data from upload response (includes artist/title/genre)
	‚Ä¢	Save Metadata nodes use "Using Fields Below" mode (not raw JSON)
	‚Ä¢	Both branches end at same Respond node

OFFICIAL PRODUCT ROADMAP
MVP (Current - Target: 2-4 Weeks)
Scope:
	‚Ä¢	Release Types: Singles only (NO EP functionality in MVP)
	‚Ä¢	User: Solo use (me only)
	‚Ä¢	Infrastructure: Local (n8n Docker, file-handler on Mac, Next.js UI local)
	‚Ä¢	Automation Level: Core uploads automated, distribution/promo mostly manual with guides
7 Milestones:
	1	‚úÖ Release Intake (n8n webhook)
	2	‚úÖ File Upload Handler (Express/Multer API)
	3	‚úÖ Metadata Transformer (releaseId generation, validation, metadata.json)
	4	‚è≥ Storage Manager (listing, disk monitoring, duplicate detection, file validation) - NEXT
	5	‚è≥ Distribution Orchestrator (SoundCloud/YouTube automated, DistroKid/Bandcamp guides)
	6	‚è≥ Promo Content Generator (social media templates)
	7	‚è≥ Next.js UI (user-facing upload form)
Success Criteria:
	‚Ä¢	‚úÖ Can upload a single track ‚Üí auto-published to SoundCloud + YouTube
	‚Ä¢	‚úÖ Understand n8n, Express APIs, file handling, webhooks, multipart/form-data
	‚Ä¢	‚úÖ Have working foundation for V2 enhancements

V2 - Professional Release Management & Maximum Label Outreach (Target: 2-3 Months After MVP)
Infrastructure Upgrades:
Hybrid Cloud Architecture:
	‚Ä¢	n8n: Migrate to n8n Cloud or self-hosted VPS for 24/7 workflow execution
	‚Ä¢	File-handler API: Deploy to cloud (AWS/Railway/Vercel/Fly.io)
	‚Ä¢	Storage: S3/Cloudflare R2 instead of local folders
	‚Ä¢	Next.js UI: Deploy to Vercel/Netlify (accessible from any device)
Multi-User Support:
	‚Ä¢	User accounts with authentication (Clerk/Auth0/Supabase Auth)
	‚Ä¢	Per-artist credential vault (encrypted storage for DistroKid/SoundCloud/YouTube logins)
	‚Ä¢	Billing integration (Stripe: freemium model with submission limits)
	‚Ä¢	Role-based access (Artist vs Manager vs Label accounts)
Storage Management Features (Deferred from MVP):
	‚Ä¢	Release archiving (move old releases to archive folder)
	‚Ä¢	Release deletion with confirmation
	‚Ä¢	Bulk operations (archive all releases older than 6 months)
	‚Ä¢	Storage optimization (compress old releases, cleanup temp files)
Business Model (V2 Launch):
	‚Ä¢	Free Tier: 1-2 releases/month, basic distribution (SoundCloud/YouTube), limited label submissions
	‚Ä¢	Pro Tier: $29-49/month, unlimited releases, all 4 label platforms, analytics, priority support
	‚Ä¢	Pay-As-You-Go: $1-5 per label submission (alternative to subscription)
	‚Ä¢	Goal: Break even with infrastructure costs, serve indie electronic producers affordably

EP Functionality (Full Implementation):
EP Parent/Child Linking:
	‚Ä¢	Create EP entities that group multiple individual tracks
	‚Ä¢	EP-level metadata: EP title, release date, EP artwork, track ordering, catalog numbers
	‚Ä¢	Individual tracks retain their own releaseIds (uploaded separately via MVP single-track workflow)
"Link Tracks to EP" Workflow:
	‚Ä¢	Post-upload workflow to bundle existing tracks into EPs
	‚Ä¢	User flow: Upload 3 tracks individually ‚Üí Run "Create EP Bundle" workflow ‚Üí Select tracks to link
	‚Ä¢	Output: EP metadata file in /releases/<epId>/ folder linking child releaseIds
Smart Distribution:
	‚Ä¢	Auto-detect if tracks belong to an EP (check for epId in metadata)
	‚Ä¢	Generate unified DistroKid pack for entire EP (not 3 separate singles)
	‚Ä¢	Coordinated release strategies:
	‚ó¶	Simultaneous: All tracks go live same day
	‚ó¶	Pre-release single: Track 1 releases 2 weeks early, full EP later
	‚Ä¢	Beatport EP submission (all tracks + EP metadata)
Why This Approach:
	‚Ä¢	Matches professional label workflows (each track = separate entity)
	‚Ä¢	Flexibility: release as singles or bundle as EP later
	‚Ä¢	Each track has unique metadata (different titles, artwork, ISRC codes)
	‚Ä¢	Better for Spotify/Apple Music (EP appears as cohesive album unit vs scattered singles)

Enhanced Label Outreach - 4 Platform Strategy:
Goal: Maximize electronic music label reach (major + independent labels)
Platform 1: LabelRadar (Existing - Beatport Group)
	‚Ä¢	Focus: Direct A&R connections, Beatport-connected labels
	‚Ä¢	Integration: Semi-automated (Playwright MCP guides user through submission)
Platform 2: DropTrack (NEW - Primary Addition)
	‚Ä¢	Focus: Multi-label demo submissions with real-time tracking
	‚Ä¢	Network: 2,400+ labels accepting demos (2026 data)
	‚Ä¢	Features: Simultaneous submissions, response analytics
	‚Ä¢	Why: Higher acceptance rates, efficiency, strong electronic genre coverage
	‚Ä¢	Pricing model: Pass-through costs to user ($15-30/month tier)
Platform 3: Groover (NEW - Secondary Addition)
	‚Ä¢	Focus: Guaranteed feedback + broader industry networking
	‚Ä¢	Network: Labels, playlist curators, bloggers, concert promoters, A&R reps
	‚Ä¢	Features: Guaranteed response within 7 days, international reach (strong EU)
	‚Ä¢	Pricing: ‚Ç¨1 per submission (Grooviz credits), pay-as-you-go
	‚Ä¢	Why: Feedback loop improves releases, playlist placement attracts labels
Platform 4: SubmitHub (NEW - Tertiary - Playlist Focus)
	‚Ä¢	Focus: Playlist placement + blog coverage (indirect label reach strategy)
	‚Ä¢	Network: Spotify/YouTube curators, TikTok influencers, music blogs
	‚Ä¢	Features: Credit-based system, curators must respond
	‚Ä¢	Why: Build streams/credibility BEFORE approaching labels (proof of traction)
Submission Strategy Workflow:
	1	User uploads track via Music Agent form
	2	Auto-submit to LabelRadar (A&R) + DropTrack (labels) + Groover (labels + curators)
	3	Optional: Use SubmitHub for playlist placements (builds stream count)
	4	Track all responses in unified dashboard (which platforms work best for user's genre)

Playwright MCP Integration:
What: Visual browser automation via Playwright Model Context Protocol + Claude Code
How It Works:
	‚Ä¢	Install Playwright MCP server in development environment
	‚Ä¢	Claude can launch browsers, navigate to platform pages (DistroKid, Bandcamp, LabelRadar, DropTrack, Groover, SubmitHub)
	‚Ä¢	Playwright takes screenshots, highlights form fields, identifies upload buttons
	‚Ä¢	Claude generates step-by-step visual instructions: "Paste artist name in this field [screenshot with arrow], upload WAV file here [highlighted button], then review and click Submit"
Automation Boundaries:
	‚Ä¢	‚úÖ Automated: Open upload pages, identify form fields, pre-fill text from releaseId metadata, generate visual instructions, screenshot current state for review
	‚Ä¢	‚ùå Manual (user clicks): Final Submit button, payment confirmations, Publish/Go Live buttons, Terms of Service acceptance, copyright attestations, any action with legal/financial consequences
Why This Balance:
	‚Ä¢	Eliminates guesswork ("Where exactly do I paste this on LabelRadar's form?")
	‚Ä¢	User retains control over legal/financial actions (safety + compliance)
	‚Ä¢	Semi-automation: prep work automated, final commitment manual
	‚Ä¢	Human-in-the-loop = industry best practice for music rights
Example Workflow:
	1	Music Agent detects release ready for distribution
	2	n8n triggers "Submit to DropTrack" task
	3	Playwright opens DropTrack submission form in browser
	4	Claude analyzes page, takes screenshot, highlights fields
	5	Claude displays instructions: "I've opened DropTrack. Here's what to do: [annotated screenshot] 1) Click 'Upload Audio' button (highlighted in red), 2) Select your WAV file from ~/Documents/Music Agent/Releases/2026-02-05_Artist_Title/audio/, 3) Paste 'Artist Name' in this field, 4) Review submission, 5) Click Submit when ready"
	6	User follows visual guide, reviews, clicks Submit

Beatport Readiness:
Enhanced Metadata Validation for Electronic Music Distribution:
"Beatport Mode" Toggle (Workflow Setting):
	‚Ä¢	When enabled: Requires label/imprint name (mandatory field, cannot proceed without it)
	‚Ä¢	Stricter genre taxonomy: Dropdown using Beatport's official 150+ genre list (not free-text input)
	‚Ä¢	Catalog number generation: Auto-generate label catalog numbers following convention (e.g., LAB001, LAB002, IMPRINT-2024-03)
	‚Ä¢	Release date constraints: Enforce minimum 2-4 week lead time (Beatport/labels require advance notice for marketing)
	‚Ä¢	Key/BPM metadata: Optional fields for DJ-focused metadata (useful for Beatport but not required for other platforms)
Distributor Strategy Toggle:
	‚Ä¢	"Fastest Release" Mode:
	‚ó¶	Use DistroKid (2-3 day turnaround)
	‚ó¶	Generic coverage (Spotify, Apple Music, Amazon, basic stores)
	‚ó¶	Best for: Quick releases, testing tracks, singles
	‚Ä¢	"Electronic Coverage" Mode:
	‚ó¶	Prioritize Beatport-compatible distributors (DistroKid still works, but recommendations change)
	‚ó¶	Slower (7-14 days) but reaches DJ-focused stores (Beatport, Traxsource, Juno Download)
	‚ó¶	Best for: Label releases, peak-time techno/house, professional DJ distribution
Why This Matters:
	‚Ä¢	Beatport = #1 platform for electronic music professionals (DJs buy tracks, labels scout talent)
	‚Ä¢	Labels require specific metadata format for Beatport submission acceptance
	‚Ä¢	Having Beatport-ready releases increases label signing rates (shows artist understands professional standards)
	‚Ä¢	Strategic trade-off: Speed vs. electronic music market penetration (user chooses based on goals)

Analytics Dashboard (V2 - Not V3):
Why V2, Not V3:
	‚Ä¢	Analytics is infrastructure, not advanced feature
	‚Ä¢	Required to validate platform choices (which submission platform converts best?)
	‚Ä¢	Informs V3 AI features (need historical data before AI can optimize)
	‚Ä¢	Enables learning loop (improve Release N+1 based on Release N performance)
	‚Ä¢	Justifies costs (ROI tracking: "Spent ‚Ç¨10 on Groover, got 2 label responses, 1 acceptance")
	‚Ä¢	Supports multi-user business model (users need dashboard to see value, justify subscription)
Data Collection:
Release Performance Metrics:
	‚Ä¢	Stream counts: Spotify, SoundCloud, YouTube (API integrations)
	‚Ä¢	Download counts: Bandcamp, Beatport (webhook/API)
	‚Ä¢	Revenue per release (aggregated from distributor reports)
	‚Ä¢	Geographic breakdown (where are listeners? useful for targeting labels by region)
	‚Ä¢	Growth trends (week-over-week, month-over-month)
Submission Tracking:
	‚Ä¢	All label/curator submissions across 4 platforms (LabelRadar, DropTrack, Groover, SubmitHub)
	‚Ä¢	Response rates per platform (e.g., "LabelRadar: 10% response rate, DropTrack: 25%, Groover: 90% guaranteed")
	‚Ä¢	Feedback scores/comments (Groover provides ratings, SubmitHub has curator notes)
	‚Ä¢	Time to response (average days until label replies)
	‚Ä¢	Conversion rate (submission ‚Üí opened ‚Üí listened ‚Üí accepted ‚Üí signed)
	‚Ä¢	Cost per acceptance (total spent √∑ number of acceptances)
Platform ROI Analysis:
	‚Ä¢	Cost per submission (SubmitHub credits, Groover Grooviz, DropTrack subscription)
	‚Ä¢	Acceptances per dollar spent (efficiency metric)
	‚Ä¢	Stream increase after playlist placements (did SubmitHub curator add lead to 10K new streams?)
	‚Ä¢	Best-performing platforms by genre (techno vs house vs ambient might have different optimal platforms)
Promo Template Performance:
	‚Ä¢	Social media post engagement (click-through rates, likes, comments, shares)
	‚Ä¢	Which templates lead to most streams (A/B test caption styles)
	‚Ä¢	Optimal posting times (when does your audience engage most?)
	‚Ä¢	Cross-platform performance (Instagram vs TikTok vs Twitter/X)
Visualization (Dashboard Design):
	‚Ä¢	Hero metrics: Total streams, total releases, submission success rate, revenue generated
	‚Ä¢	Trend graphs: Stream growth over time, weekly release cadence
	‚Ä¢	Comparative charts: "Your techno releases get 40% more engagement than house releases"
	‚Ä¢	Actionable insights: "Your DropTrack submissions on Tuesdays have 2x response rate vs weekends"
	‚Ä¢	Platform comparison table: Side-by-side ROI for each submission platform

V2 Success Criteria:
	‚Ä¢	‚úÖ Can process EP releases (3+ tracks grouped as cohesive unit)
	‚Ä¢	‚úÖ Automated submissions to 4 platforms (LabelRadar, DropTrack, Groover, SubmitHub)
	‚Ä¢	‚úÖ Dashboard shows which platforms work best for user's genre
	‚Ä¢	‚úÖ System runs 24/7 in cloud (workflows execute even when computer is off)
	‚Ä¢	‚úÖ Can onboard a second user (friend/beta tester) with their own account
	‚Ä¢	‚úÖ Playwright MCP guides user through all upload forms visually (semi-automated)
	‚Ä¢	‚úÖ Release archiving and deletion features fully functional

V3 - AI-Enhanced Workflow Optimization (Target: Ongoing After V2 Stabilizes)
Claude CoWork for Feature Development:
What: Use Claude CoWork (Anthropic's team collaboration AI workspace) for developing new Music Agent features
When to Start: Beginning of V2 planning phase (NOT in MVP)
Why V2 Timing:
	‚Ä¢	By V2, you'll have real user feedback, analytics data, beta user quotes
	‚Ä¢	CoWork excels at synthesizing patterns from multiple sources (upload 50 user comments, Claude identifies top 5 feature requests)
	‚Ä¢	Complex V2 features (EP linking, multi-platform submissions, analytics schema) benefit from persistent architectural discussions
	‚Ä¢	Enough context accumulated to make CoWork's persistent memory valuable
	‚Ä¢	You'll have revenue to justify tool cost (~$30-40/month on top of Claude Pro)
How It Works:
	‚Ä¢	Shared projects: Create "Music Agent V2 Features" workspace in CoWork
	‚Ä¢	Real-time collaboration: You + Claude brainstorm features together across multiple sessions
	‚Ä¢	Persistent conversation history: Full context of architecture decisions preserved (why did we choose this database schema?)
	‚Ä¢	Knowledge base: Upload user feedback CSVs, analytics screenshots, competitor research docs, API documentation
	‚Ä¢	Version control for ideas: Track feature evolution over weeks/months ("We considered Approach A, but chose B because...")
V2 Use Cases for CoWork:
	1	EP Linking Feature Design:
	‚ó¶	Upload: Current releaseId data structure, user requests for EP grouping
	‚ó¶	Discussion: "How to link tracks without breaking existing single-track releases?"
	‚ó¶	Output: Database schema, n8n workflow modifications, migration plan for existing data
	2	Multi-Platform Submission Logic:
	‚ó¶	Upload: LabelRadar/DropTrack/Groover API docs, rate limits, pricing models
	‚ó¶	Discussion: "How to batch submissions? Handle API failures? Track per-user costs?"
	‚ó¶	Output: Submission orchestrator design, error handling strategy, retry logic
	3	Analytics Schema Design:
	‚ó¶	Upload: Sample user feedback ("I want to know which labels respond fastest to my genre")
	‚ó¶	Discussion: "What data to collect? How to visualize? Privacy concerns with storing label response data?"
	‚ó¶	Output: PostgreSQL table schemas, dashboard mockup, data retention policies
	4	Pricing Strategy:
	‚ó¶	Upload: Competitor pricing (DistroKid $19.99/year, Groover ‚Ç¨1/submission), your cost structure
	‚ó¶	Discussion: "Free tier limits? Pro tier features? Subscription vs pay-per-submission?"
	‚ó¶	Output: Pricing tiers table, feature matrix, user value propositions
	5	Playwright MCP Integration Planning:
	‚ó¶	Upload: Screenshots of DistroKid/LabelRadar upload forms
	‚ó¶	Discussion: "What to automate? What stays manual? How to handle form changes when platforms update?"
	‚ó¶	Output: Automation boundaries, user experience flow, error recovery procedures
Why CoWork Over Regular Claude:
	‚Ä¢	Persistent memory: Remembers decisions across days/weeks (regular Claude forgets after session ends)
	‚Ä¢	Shared context: If you bring in a co-founder/contractor later, they can read full CoWork history (instant onboarding)
	‚Ä¢	Version control: Can reference past discussions ("In Session 3, we decided to use PostgreSQL because...")
	‚Ä¢	Knowledge base: Upload 50 files once, Claude has context for all future conversations
What NOT to Use CoWork For (Use Regular Claude/Perplexity Instead):
	‚Ä¢	‚ùå MVP implementation questions ("How does Multer work?") ‚Üí Use Perplexity (needs web search)
	‚Ä¢	‚ùå Quick debugging ("What's this error?") ‚Üí Use Claude Code in Cursor (real-time coding)
	‚Ä¢	‚ùå One-off research ("Compare S3 vs R2 pricing") ‚Üí Use Perplexity (needs current data)

Musician-First Features (Claude CoWork Co-Designed):
Context for Feature Design:‚Ä®Target user = Busy electronic music producer who:
	‚Ä¢	Releases 1-2 tracks per week (high output, quality over quantity)
	‚Ä¢	Juggles: music production, promotion, live gigs/DJing, day job
	‚Ä¢	Wants maximum label visibility + stream growth with minimum manual work
	‚Ä¢	Values: Each release matters (not spamming), professional presentation, data-driven improvement
	‚Ä¢	Needs: Clear ROI on time/money spent on promotion
Feature Ideas (To Develop with Claude CoWork in V3):
1. Smart Release Timing Optimizer
	‚Ä¢	Analyze user's past release performance (best/worst performing days)
	‚Ä¢	Cross-reference with industry data (Beatport's "New Release" spotlights, Spotify's Fresh Finds schedule)
	‚Ä¢	Suggest optimal release days/times for user's specific genre
	‚Ä¢	Avoid major competitor releases (don't drop techno track same day as Tale Of Us or Amelie Lens)
	‚Ä¢	Example output: "Based on your data, release on Wednesday 6am EST. Your past Wednesday releases averaged 40% more first-week streams. Avoid Feb 14 (Valentine's Day = low electronic music engagement)."
2. Promo Template Evolution Engine
	‚Ä¢	A/B test different social media caption styles automatically
	‚Ä¢	Track: Which captions drive most streams? Which get most engagement?
	‚Ä¢	Learn what works for YOUR audience specifically (not generic advice)
	‚Ä¢	Auto-generate variations based on winning patterns
	‚Ä¢	Example: "Your captions with üéµ emoji + BPM + genre tags get 3x more clicks. Generated 5 new variations following this pattern."
3. Label Relationship Manager (CRM for Musicians)
	‚Ä¢	Track all label submissions across platforms (who, when, what response)
	‚Ä¢	Set automatic follow-up reminders (3 weeks after submission, 1 month after acceptance)
	‚Ä¢	Score labels by: Response rate, acceptance rate, post-release support quality
	‚Ä¢	Prioritize labels matching user's style (based on accepted/rejected history over time)
	‚Ä¢	Avoid duplicate submissions (flag if you already submitted to this label 2 months ago)
	‚Ä¢	Example: "You've submitted to Label X 3 times, no response. Suggest focusing on Label Y (similar style, 40% acceptance rate for your genre)."
4. Cross-Platform Brand Sync
	‚Ä¢	Detect achievements (hit 10K streams, got signed, played at festival)
	‚Ä¢	Auto-update all platform bios simultaneously (SoundCloud, Spotify, Instagram, Beatport artist page)
	‚Ä¢	Sync release announcements across platforms (one-click distribution of announcement)
	‚Ä¢	Maintain consistent artist brand everywhere (same bio style, links, achievements)
	‚Ä¢	Example: "Detected: Your track hit 50K streams on Spotify. Updated your Instagram/SoundCloud bios with this milestone. Posted celebration story."
5. Intelligent Promo Campaign Orchestrator
	‚Ä¢	Suggest influencers/curators based on similar artists' growth patterns
	‚Ä¢	Automate DM outreach with user approval (generate personalized messages, user reviews before sending)
	‚Ä¢	Schedule teaser content leading up to release (7 days before: snippet, 3 days: artwork reveal, day of: full release)
	‚Ä¢	Coordinate across platforms (Instagram Stories, TikTok, Twitter/X, YouTube Shorts)
	‚Ä¢	Example: "Campaign plan for next release: Week -2: Post studio snippet on IG. Week -1: TikTok teaser with BPM challenge. Day 0: Coordinate YouTube premiere + Instagram Live listening party."
6. Collaboration Finder & Remix Exchange
	‚Ä¢	Match user with producers at similar skill/following level for collabs
	‚Ä¢	Suggest remix opportunities based on compatible styles (user makes melodic techno ‚Üí suggest remixing progressive house artists)
	‚Ä¢	Track collab success rates (do collaborations actually grow your audience?)
	‚Ä¢	Facilitate remix exchanges (you remix Artist A, they remix you)
	‚Ä¢	Example: "Found 5 producers in Berlin making similar melodic techno, 5K-15K followers, open to collabs. Suggested outreach: [personalized message templates]."

V3 Success Criteria:
	‚Ä¢	‚úÖ AI suggests optimal release timing based on user's historical data + industry trends
	‚Ä¢	‚úÖ Promo templates auto-improve each release (learn from performance)
	‚Ä¢	‚úÖ System learns which labels match user's style (recommendation engine)
	‚Ä¢	‚úÖ 50%+ reduction in manual promo work (time saved per release)
	‚Ä¢	‚úÖ Claude CoWork used to design all new features (persistent context, collaborative planning)
	‚Ä¢	‚úÖ Users report: "Music Agent feels like having a manager + publicist"

Technology Evolution Across Phases
Component
MVP
V2
V3
n8n Workflows
Docker (local Mac)
n8n Cloud or VPS
Same + AI-powered nodes
File-handler API
Express on Mac (port 3001)
Cloud deploy (Railway/Fly.io)
Serverless functions (edge computing)
Storage
Local folders (~/Documents/Music Agent/Releases/)
S3/Cloudflare R2
Same + CDN for global access
Frontend UI
Next.js (local dev)
Vercel production deploy
Same + AI chat interface
Authentication
None (solo use)
Clerk/Auth0/Supabase Auth
Same + SSO for labels
Database
None (file-based metadata)
PostgreSQL (Supabase/Neon)
Same + vector DB for AI features
Automation Level
n8n only, manual distribution
n8n + Playwright MCP (semi-auto)
n8n + MCP + AI agents
Development Tools
Cursor + Claude Code + Perplexity
Same + Claude CoWork (V2 planning)
Same (CoWork for all features)
Monitoring
Terminal logs only
Application monitoring (Sentry/LogRocket)
Same + AI anomaly detection




Key Technical Decisions & Rationale
releaseId Format: YYYY-MM-DD_ArtistName_TrackTitle
	‚Ä¢	Example: 2026-02-05_SophieJoe_TellMe
	‚Ä¢	Why: Sortable by date, human-readable, filesystem-safe, unique per release
	‚Ä¢	Spaces removed from artist and title for filesystem compatibility
Genre Dropdown (Not Free Text)
	‚Ä¢	10 predefined options in alphabetical order
	‚Ä¢	Options: Ambient, Deep House, House, Indie Dance, Melodic House and Techno, Progressive House, Tech House, Techno, Trance, Other
	‚Ä¢	Why: Ensures data consistency, prevents typos, easier for analytics later
Metadata in Upload Response
	‚Ä¢	File-handler returns artist/title/genre in POST /upload response
	‚Ä¢	Why: Allows metadata generation without referencing previous nodes by name
	‚Ä¢	Avoids "Referenced node doesn't exist" errors in n8n Code nodes
express.json() Middleware
	‚Ä¢	Added to server.js for parsing JSON request bodies
	‚Ä¢	Why: Required for POST /metadata endpoint to read req.body
	‚Ä¢	Without it: req.body is undefined, causing errors
n8n HTTP Request: "Using Fields Below" for JSON
	‚Ä¢	Don't use raw JSON with {{ }} expressions in quotes
	‚Ä¢	Use parameter fields with expression mode instead
	‚Ä¢	Why: Prevents "[object Object]" serialization errors
Parallel Branch Strategy in n8n
	‚Ä¢	Duplicate metadata nodes for with-video and no-video branches
	‚Ä¢	Don't use Merge node (causes execution issues with conditional IF)
	‚Ä¢	Why: Simpler, more reliable, easier to debug
Storage Structure
	‚Ä¢	Example: 2026-02-05_SophieJoe_TellMe/audio/track.wav, /artwork/cover.jpg, /video/promo.mp4, /metadata.json
	‚Ä¢	Why: Keeps all release assets together, easy to zip for distribution, clear organization
No Database in MVP
	‚Ä¢	Store metadata as JSON files in release folders
	‚Ä¢	Why: Simpler for learning, fewer moving parts, easy to inspect/debug
	‚Ä¢	V2 adds PostgreSQL when multi-user requires querying/relationships

Common Issues & Solutions (Reference)
Issue: ECONNREFUSED when n8n calls file-handler
	‚Ä¢	Solution: Use http://host.docker.internal:3001 (not 127.0.0.1) in n8n HTTP Request node
	‚Ä¢	Why: Docker networking isolation
Issue: "Referenced node doesn't exist" in n8n Code node
	‚Ä¢	Solution: Don't use $('Node Name') to reference other nodes; use data from $input or pass data through workflow
	‚Ä¢	Why: Node name references are fragile and case-sensitive
Issue: req.body is undefined in Express
	‚Ä¢	Solution: Add app.use(express.json()); before route definitions
	‚Ä¢	Why: Express doesn't parse JSON bodies by default
Issue: metadata.json shows "[object Object]"
	‚Ä¢	Solution: In n8n HTTP Request, use "Using Fields Below" mode, not raw JSON with quoted expressions
	‚Ä¢	Why: Quoted expressions serialize objects to strings
Issue: Merge node not executing in n8n
	‚Ä¢	Solution: Don't use Merge after conditional IF‚Äîduplicate nodes on each branch instead
	‚Ä¢	Why: Merge expects both inputs, but IF only sends one
Issue: Genre showing "Unknown" in metadata
	‚Ä¢	Solution: Pass genre as query parameter in HTTP Request, return it in file-handler response
	‚Ä¢	Why: Metadata code can't access Form Trigger data directly

Learning Preferences & Communication Style
How to Help Me
Explain WHY we do each step (not just commands to copy-paste)
	‚Ä¢	Good: "We use express.json() because Express doesn't parse JSON request bodies by default. Without it, req.body will be undefined when the client sends JSON data."
	‚Ä¢	Bad: "Add app.use(express.json()); to your code."
No # comments in terminal commands (they break copy-paste in zsh)
	‚Ä¢	Good: Separate explanation from command
	‚Ä¢	Bad: npm install express # This installs Express
Assume I'm a total beginner, explain technical terms in plain language
	‚Ä¢	Use real-world analogies when helpful
	‚Ä¢	Define jargon on first use (e.g., "API endpoint = a specific URL your server listens to, like a phone number")
Small milestones with clear checkpoints (celebrate small wins)
	‚Ä¢	Break features into 5-10 minute tasks
	‚Ä¢	Test each piece as we build (don't write 500 lines before testing)
	‚Ä¢	Clear "‚úÖ Success looks like..." criteria for each step
Test each piece as we build
	‚Ä¢	Write code ‚Üí test ‚Üí debug ‚Üí move forward
	‚Ä¢	Never write more than 20-30 lines without running it
	‚Ä¢	Show me how to verify each step works
Real-world analogies when explaining complex concepts
	‚Ä¢	Example: "Multer is like a mail sorter at a post office‚Äîit receives all incoming packages (files) and sorts them into different bins (folders) based on what's inside."

Tools I'm Learning
Cursor IDE (primary code editor)
	‚Ä¢	AI-powered code completion
	‚Ä¢	Integrated with Claude Code for real-time help
	‚Ä¢	Used for all file editing
Claude Code (AI pair programmer, real-time help)
	‚Ä¢	Current use (MVP): Secondary tool, use Perplexity for Milestones 4-6
	‚Ä¢	Future use (V2): Primary development assistant
	‚Ä¢	V3: Combined with Claude CoWork for feature planning
	‚Ä¢	Can see full codebase context
	‚Ä¢	Real-time debugging and code generation
Perplexity (research + long conversations, current web data access)
	‚Ä¢	Current use (MVP): Primary assistant for Milestones 1-6
	‚Ä¢	Long-form context (master prompts like this one)
	‚Ä¢	Research latest packages, best practices, current web data
	‚Ä¢	Planning and architecture discussions
GitHub Desktop (version control, beginner-friendly)
	‚Ä¢	Visual Git interface (no command line needed)
	‚Ä¢	Commit, push, branch management
	‚Ä¢	Easier than terminal Git commands
Terminal (getting comfortable with command line)
	‚Ä¢	Basic commands: cd, ls, mkdir, npm install, node server.js
	‚Ä¢	Understanding output (logs, errors, success messages)
	‚Ä¢	Copy-paste friendly (no inline comments)
n8n (visual workflow builder, learning automation concepts)
	‚Ä¢	No-code workflow automation
	‚Ä¢	Learning: HTTP requests, webhooks, data transformation, conditional logic
	‚Ä¢	Bridging UI actions to backend API calls

Current Learning Goals
MVP Phase (Now - Next 2-4 Weeks):
	‚Ä¢	Master file system operations (reading directories, checking file stats, moving files)
	‚Ä¢	Understand async/await patterns (why we need it, how to handle errors)
	‚Ä¢	Learn API design patterns (REST conventions, error responses, status codes)
	‚Ä¢	Build confidence with Node.js/Express fundamentals
	‚Ä¢	Get comfortable debugging (reading error messages, console.log strategies)
V2 Phase (2-3 Months After MVP):
	‚Ä¢	Transition to Claude Code as primary development assistant
	‚Ä¢	Learn cloud deployment (Railway, Vercel, understanding environment variables)
	‚Ä¢	Understand authentication/authorization concepts (JWT tokens, sessions, protected routes)
	‚Ä¢	Database fundamentals (PostgreSQL, schema design, relationships, migrations)
	‚Ä¢	Start using Claude CoWork for feature planning
	‚Ä¢	Multi-user systems (how to isolate user data, billing integration basics)
V3 Phase (Ongoing):
	‚Ä¢	AI/ML integration patterns (when to use AI, how to structure prompts programmatically)
	‚Ä¢	Advanced n8n workflows (complex branching, error recovery, rate limiting)
	‚Ä¢	System architecture (microservices vs monolith, when to split services)
	‚Ä¢	Performance optimization (caching, CDN, database indexing)
	‚Ä¢	Product thinking (prioritization, user feedback loops, metric-driven development)

Tool-Specific Instructions
When Using Perplexity (Current - MVP Milestones 4-6):
	‚Ä¢	Paste this full Master Prompt at start of new thread
	‚Ä¢	Ask for explanations with examples
	‚Ä¢	Request step-by-step breakdowns for complex tasks
	‚Ä¢	Use for research (latest npm packages, best practices, comparing approaches)
	‚Ä¢	Good for: Planning, research, learning concepts, troubleshooting
When Using Claude Code (Starting V2):
	‚Ä¢	Share full codebase context (Cursor integration handles this)
	‚Ä¢	Ask for real-time debugging help
	‚Ä¢	Request code generation with explanations
	‚Ä¢	Use for: Writing code, debugging errors, refactoring, testing
	‚Ä¢	Good for: Implementation, quick fixes, code review
When Using Claude CoWork (Starting V2 Planning):
	‚Ä¢	Create "Music Agent V2 Features" project
	‚Ä¢	Upload: User feedback, API docs, analytics screenshots, competitor research
	‚Ä¢	Use for long-term feature discussions (persist across days/weeks)
	‚Ä¢	Reference past decisions ("In our Pricing Strategy session, we decided...")
	‚Ä¢	Good for: Feature design, architecture decisions, business strategy, persistent context
When to Use Which Tool:
Task
Use
Why
"What's the latest best practice for X?"
Perplexity
Needs web search, current data
"Fix this error in my code"
Claude Code
Has codebase context, real-time
"How should we design EP linking feature?"
Claude CoWork
Long-term planning, persistent memory
"Write this function for me"
Claude Code
Real-time code generation
"Compare these 3 approaches"
Perplexity
Research, web data, examples
"Remember our pricing decision from last week?"
Claude CoWork
Persistent conversation history




Business Model & Goals
Philosophy: Break even with infrastructure costs, serve indie electronic producers affordably
Pricing Strategy (V2 Launch)
Free Tier:
	‚Ä¢	1-2 releases per month
	‚Ä¢	Basic distribution (SoundCloud + YouTube automated)
	‚Ä¢	Limited label submissions (2 submissions/month across all platforms)
	‚Ä¢	Basic analytics (stream counts only)
Pro Tier ($29-49/month):
	‚Ä¢	Unlimited releases (supports 1-2 tracks per week output)
	‚Ä¢	All 4 label platforms (LabelRadar, DropTrack, Groover, SubmitHub)
	‚Ä¢	Full analytics dashboard + insights
	‚Ä¢	Playwright MCP visual guidance
	‚Ä¢	Priority support
	‚Ä¢	Advanced features (EP grouping, Beatport mode, promo templates, release archiving, bulk deletion)
Pay-As-You-Go (Alternative to Pro):
	‚Ä¢	$1-5 per label submission (no subscription)
	‚Ä¢	Good for infrequent releasers (1 release every 3 months)
	‚Ä¢	Pay only for what you use
Goal: Break even with infrastructure costs, build sustainable tool for indie producers

Master Prompt Management
File Location

text
~/Documents/music-agent-mvp/docs/MASTER_PROMPT.md
Archive Before Updating

bash
cd ~/Documents/music-agent-mvp/docs
cp MASTER_PROMPT.md archive/MASTER_PROMPT_$(date +%Y-%m-%d_%H-%M).md
Quick Access Alias (Already Set Up)

bash
prompt
What it does: Copies entire Master Prompt to clipboard ‚Üí ready to paste into new Perplexity/Claude thread
When to Update This Prompt
After completing each milestone:
	‚Ä¢	Mark milestone as ‚úÖ Complete
	‚Ä¢	Add "Milestone N Complete" details to Current State section
	‚Ä¢	Archive old version with timestamp
	‚Ä¢	Commit to Git with message: "docs: Update Master Prompt - Milestone N complete"
When making architectural decisions:
	‚Ä¢	Add to "Key Technical Decisions & Rationale" section
	‚Ä¢	Document WHY you chose this approach
	‚Ä¢	Include alternatives considered and reasons for rejection
When encountering new issues/solutions:
	‚Ä¢	Add to "Common Issues & Solutions" section
	‚Ä¢	Include error message, solution, and explanation
When roadmap changes:
	‚Ä¢	Update OFFICIAL PRODUCT ROADMAP section
	‚Ä¢	Move features between MVP/V2/V3 as priorities shift
	‚Ä¢	Document reasons for changes

How to Use This Prompt
Starting a new Perplexity thread:
	1	Run prompt in terminal
	2	Paste into new Perplexity thread
	3	Ready to continue from where you left off
Starting a new Claude Code session:
	1	Cursor automatically shares codebase context
	2	Paste relevant sections (Current State, Milestone goals, Learning Preferences)
	3	Less verbose than full prompt (Claude Code has code context already)
Starting Claude CoWork (V2+):
	1	Create "Music Agent" project
	2	Upload this Master Prompt as foundational document
	3	Add user feedback, analytics, API docs as separate files
	4	Reference in conversations: "See Master Prompt section on EP Linking"
Onboarding a co-founder/contractor:
	‚Ä¢	Share this prompt
	‚Ä¢	They get full context in 10 minutes of reading
	‚Ä¢	No need for long explanations or repeated questions
Mid-milestone context refresh:
	‚Ä¢	Re-paste relevant sections only (not full prompt)
	‚Ä¢	Example: "Here's the Milestone 4 build order from my Master Prompt..."

Next Steps
Immediate: Start Milestone 4 (Storage Manager)
	1	Create branch: git checkout -b milestone-4-storage-manager
	2	Install packages: npm install music-metadata check-disk-space file-type-checker
	3	Begin with Step 1: Release Listing (GET /releases endpoint)
Short-term: Complete Milestones 4-7 (finish MVP)
Mid-term: Plan V2 architecture, start Claude CoWork experimentation
Long-term: Launch V2, onboard beta users, iterate based on feedback

Document Version: 2.2‚Ä®Last Updated: February 6, 2026, 1:09 AM CET‚Ä®Status: Milestones 1-3 Complete | Milestone 4 (Storage Manager) NEXT‚Ä®Maintained By: You (update after each milestone)